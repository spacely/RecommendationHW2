{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.app.name', 'Recommendation App'), ('spark.driver.port', '63115'), ('spark.rdd.compress', 'True'), ('spark.app.startTime', '1653276693639'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.executor.id', 'driver'), ('spark.submit.deployMode', 'client'), ('spark.driver.host', 'localhost'), ('spark.app.id', 'local-1653276695198'), ('spark.ui.showConsoleProgress', 'true'), ('spark.sql.warehouse.dir', 'file:/Users/saheedadepoju/Documents/CSE272/HW2Recommendation/spark-warehouse')]\n",
      "[('spark.executor.memory', '2g'), ('spark.driver.port', '63115'), ('spark.app.name', 'HW2 Recommendation'), ('spark.executor.id', 'driver'), ('spark.driver.host', 'localhost'), ('spark.default.parallelism', '300'), ('spark.driver.memory', '16g'), ('spark.app.id', 'local-1653276695198'), ('spark.executor.cores', '4'), ('spark.cores.max', '10'), ('spark.kryoserializer.buffer.max', '1g'), ('spark.rdd.compress', 'True'), ('spark.app.startTime', '1653276693639'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.sql.shuffle.partitions', '300'), ('spark.ui.showConsoleProgress', 'true'), ('spark.sql.warehouse.dir', 'file:/Users/saheedadepoju/Documents/CSE272/HW2Recommendation/spark-warehouse')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Recommendation App\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .getOrCreate()\n",
    "default_conf = spark.sparkContext._conf.getAll()\n",
    "print(default_conf)\n",
    "\n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '2g'),\n",
    "                                        ('spark.app.name', 'HW2 Recommendation'),\n",
    "                                        ('spark.executor.cores', '4'),\n",
    "                                        ('spark.cores.max', '10'),\n",
    "                                        ('spark.driver.memory','16g'),\n",
    "                                        ('spark.kryoserializer.buffer.max','1g'),\n",
    "                                        ('spark.default.parallelism','300'),\n",
    "                                       ('spark.sql.shuffle.partitions','300')])\n",
    "v = spark.sparkContext._conf.getAll()\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_ratings_df = spark.read.json(\"/Users/saheedadepoju/Documents/CSE272/HW2/Movies_and_TV.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_ratings_df_subset = movies_ratings_df.limit(100000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_asin_index_subset=movies_ratings_df_subset.select(\"asin\").distinct().withColumn(\"asin_index\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_training_data_merged_1 = movies_ratings_df_subset.join(movies_asin_index_subset.select('asin', 'asin_index'), ['asin'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_review_subset = movies_training_data_merged_1.select(\"reviewerID\").distinct().withColumn(\"reviewerID_index\", monotonically_increasing_id())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_training_data_merged_2 = movies_training_data_merged_1.join(movies_review_subset.select('reviewerID', 'reviewerID_index'), ['reviewerID'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training,test)=movies_training_data_merged_2.randomSplit([0.8, 0.2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "als=ALS(maxIter=5,regParam=0.09,rank=25,userCol=\"reviewerID_index\",itemCol=\"asin_index\",ratingCol=\"overall\",coldStartStrategy=\"drop\",nonnegative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=als.fit(training)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"overall\",predictionCol=\"prediction\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_1=RegressionEvaluator(metricName=\"mae\",labelCol=\"overall\",predictionCol=\"prediction\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE=1.9789857184519146\n"
     ]
    }
   ],
   "source": [
    "predictions=model.transform(test)\n",
    "rmse=evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"RMSE=\"+str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE=1.6459459560664045\n"
     ]
    }
   ],
   "source": [
    "mae = evaluator_1.evaluate(predictions)\n",
    "print(\"MAE=\"+str(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/opt/apache-spark/libexec/python/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "recs=model.recommendForAllUsers(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrecs=recs.recommendations.apply(pd.Series) \\\n",
    "            .merge(recs, right_index = True, left_index = True) \\\n",
    "            .drop([\"recommendations\"], axis = 1) \\\n",
    "            .melt(id_vars = ['reviewerID_index'], value_name = \"recommendation\") \\\n",
    "            .drop(\"variable\", axis = 1) \\\n",
    "            .dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrecs=nrecs.sort_values('reviewerID_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrecs=pd.concat([nrecs['recommendation'].apply(pd.Series), nrecs['reviewerID_index']], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrecs.columns = [\n",
    "\n",
    "        'ProductID_index',\n",
    "        'Rating',\n",
    "        'UserID_index'\n",
    "\n",
    "     ]\n",
    "md=movies_training_data_merged_2.select(movies_training_data_merged_2['reviewerID'],movies_training_data_merged_2['reviewerID_index'],movies_training_data_merged_2['asin'],movies_training_data_merged_2['asin_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saheedadepoju/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          reviewerID|     recommendations|\n",
      "+--------------------+--------------------+\n",
      "|A0025276XH785Y75YRN0|[{0767000021, 1.6...|\n",
      "|A0033358Q08LR9V17C3X|[{0780625390, 6.1...|\n",
      "|A0034986DWR7WEDQN0GV|[{0310894913, 10....|\n",
      "| A0040714X0G8QUCER7Q|[{000503860X, 5.0...|\n",
      "|A0056274FAHZQC4N2ZN8|[{0310691281, 10....|\n",
      "|A0093751ZA04WDR6FGNX|[{000503860X, 11....|\n",
      "|A0160612BLIWRHROHLLE|[{078062386X, 14....|\n",
      "|A0297244750EW7S81VID|[{0764008722, 11....|\n",
      "|A0322174KPHFYVAJWTR2|[{0784011915, 9.1...|\n",
      "|A0351505SE8094H4NC6F|[{0740318764, 5.9...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "md=md.toPandas()\n",
    "dict1 =dict(zip(md['reviewerID_index'],md['reviewerID']))\n",
    "dict2=dict(zip(md['asin_index'],md['asin']))\n",
    "nrecs['reviewerID']=nrecs['UserID_index'].map(dict1)\n",
    "nrecs['asin']=nrecs['ProductID_index'].map(dict2)\n",
    "nrecs=nrecs.sort_values('reviewerID')\n",
    "nrecs.reset_index(drop=True, inplace=True)\n",
    "new=nrecs[['reviewerID','asin','Rating']]\n",
    "new['recommendations'] = list(zip(new.asin, new.Rating))\n",
    "res=new[['reviewerID','recommendations']]\n",
    "res_new=res['recommendations'].groupby([res.reviewerID]).apply(list).reset_index()\n",
    "review_df = spark.createDataFrame(res_new)\n",
    "review_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72193"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- recommendations: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _1: string (nullable = true)\n",
      " |    |    |-- _2: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_review_df = review_df.limit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "CSV data source does not support array<struct<_1:string,_2:double>> data type.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e655035db74c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaved_review_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recommendation.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m    953\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0morc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionBy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: CSV data source does not support array<struct<_1:string,_2:double>> data type."
     ]
    }
   ],
   "source": [
    "saved_review_df.write.csv(\"recommendation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
